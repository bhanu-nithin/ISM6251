{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7fe023",
   "metadata": {},
   "source": [
    "## Assignment 1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b62d5e",
   "metadata": {},
   "source": [
    "## Model fitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a53a4",
   "metadata": {},
   "source": [
    "### importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5259f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd7c29",
   "metadata": {},
   "source": [
    "## Loading the preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbea0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"default_train_df.csv\")\n",
    "test_df=pd.read_csv(\"default_test_df.csv\")\n",
    "X_train=pd.read_csv(\"default_train_X.csv\")\n",
    "X_test=pd.read_csv(\"default_test_X.csv\")\n",
    "y_train=pd.read_csv(\"default_train_y.csv\")\n",
    "y_test=pd.read_csv(\"default_test_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562d4708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEFAULT PAYMENT NEXT MONTH\n",
       "0                             16364\n",
       "1                              4636\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1c435",
   "metadata": {},
   "source": [
    "*** There is a huge imbalance in the trained data so we need to perform a data imbalance techniqu in order to encounter the biased results ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e305c",
   "metadata": {},
   "source": [
    "## Data imbalancing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb04e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'DEFAULT PAYMENT NEXT MONTH': 1})\n"
     ]
    }
   ],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf69c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEFAULT PAYMENT NEXT MONTH\n",
       "0                             4636\n",
       "1                             4636\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb2a77",
   "metadata": {},
   "source": [
    "## Standardizing the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df673530",
   "metadata": {},
   "source": [
    "Here we will be standardizing the variables of each attribute in order to reduce the differences among them so that we will be able to predict the scores accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6901e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the predictors of training and test sets\n",
    "X_train = scaler.transform(X_train) \n",
    " \n",
    "\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce9e6d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1594649 ,  1.59086926,  0.83370498, ...,  1.48177835,\n",
       "        -0.22219406, -0.22989422],\n",
       "       [ 0.87887396, -0.50438208,  0.83370498, ..., -0.10248862,\n",
       "         0.01905733, -0.22452629],\n",
       "       [-0.24172336, -0.42379549, -1.19946507, ..., -0.29645833,\n",
       "        -0.07275223, -0.28920017],\n",
       "       ...,\n",
       "       [ 0.50824613,  0.22089723,  0.83370498, ..., -0.29645833,\n",
       "        -0.28920834, -0.28920017],\n",
       "       [-1.40542505, -0.42379549, -1.19946507, ..., -0.29645833,\n",
       "        -0.24424176, -0.245804  ],\n",
       "       [ 0.25914704,  0.14031064,  0.83370498, ..., -0.22405455,\n",
       "        -0.28920834,  2.08297309]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be534acb",
   "metadata": {},
   "source": [
    "### What is the best evaluating matrix????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140cd698",
   "metadata": {},
   "source": [
    "Our aim for this analysis is to predict accurately the default payments of a transaction. So we will be dealing with both the True Positives and False Negitives of the problem because, True positives(TN) gives number of times the model correctly predicts a default payment whereas False Negitives(FN) gives the number of times the model incorrectly predicts a non-default payment when the actual payment is a default. False negitives are also as important as True Negitives in order to define this model s accurate one.\n",
    "\n",
    "Recall is a predictive metric that deals with both true positives and false negatives. The proportion of true positives among all actual positive observations is measured by recall. It indicates how well the model can identify positive cases.\n",
    "\n",
    "Formula :\n",
    "Recall = True Positives / (True Positives + False Negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ccf6b",
   "metadata": {},
   "source": [
    "## Modelling the data with various modelling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7dd3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754eb6e8",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "### Using Random search and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ef4c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.65401857121601\n",
      "... with parameters: {'min_samples_split': 41, 'min_samples_leaf': 85, 'min_impurity_decrease': 0.0001, 'max_leaf_nodes': 84, 'max_depth': 20, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,25), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "best_DTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc29d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1024 candidates, totalling 5120 fits\n",
      "The best recall score is 0.65401857121601\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 18, 'max_leaf_nodes': 82, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 85, 'min_samples_split': 39}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "min_samples_split = rand_search.best_params_['min_samples_split']\n",
    "min_samples_leaf = rand_search.best_params_['min_samples_leaf']\n",
    "min_impurity_decrease = rand_search.best_params_['min_impurity_decrease']\n",
    "max_leaf_nodes = rand_search.best_params_['max_leaf_nodes']\n",
    "max_depth = rand_search.best_params_['max_depth']\n",
    "criterion = rand_search.best_params_['criterion']\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(min_samples_split-2,min_samples_split+2),  \n",
    "    'min_samples_leaf': np.arange(min_samples_leaf-2,min_samples_leaf+2),\n",
    "    'min_impurity_decrease': np.arange(min_impurity_decrease-0.0001, min_impurity_decrease+0.0001, 0.00005),\n",
    "    'max_leaf_nodes': np.arange(max_leaf_nodes-2,max_leaf_nodes+2), \n",
    "    'max_depth': np.arange(max_depth-2,max_depth+2), \n",
    "    'criterion': [criterion]\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_DTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b369b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7301111 Precision=0.4249738 Recall=0.6075000 F1=0.5001029\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "Recall_Dtree= {TP/(TP+FN)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be63b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Decision Tree\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203a84e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.730111</td>\n",
       "      <td>0.424974</td>\n",
       "      <td>0.6075</td>\n",
       "      <td>0.500103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  Accuracy  Precision  Recall        F1\n",
       "0  Decision Tree  0.730111   0.424974  0.6075  0.500103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecfecb",
   "metadata": {},
   "source": [
    "###  Logistic Regression\n",
    "### using Random search and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b6e9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "585 fits failed out of a total of 1500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "177 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "222 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "186 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.6251054  0.62532115        nan 0.62424282        nan\n",
      " 0.62402707 0.6251054  0.62812436 0.62575251        nan 0.62532115\n",
      " 0.62726164        nan 0.6251054         nan 0.6251054         nan\n",
      "        nan 0.62532115 0.62726164        nan        nan        nan\n",
      "        nan 0.6251054  0.6251054  0.62532115        nan        nan\n",
      "        nan 0.62510554        nan        nan        nan 0.62575251\n",
      " 0.6251054  0.55672836 0.62532115        nan 0.61885089 0.6251054\n",
      " 0.6251054  0.62532115        nan 0.6251054  0.6251054  0.62532115\n",
      " 0.6251054  0.62532115 0.62510554 0.62510554        nan 0.6251054\n",
      "        nan 0.55715972 0.62532115 0.6251054  0.62510554 0.62532115\n",
      " 0.62510554        nan        nan        nan 0.62726164 0.6251054\n",
      "        nan 0.62036044        nan        nan 0.62532115 0.55715972\n",
      "        nan 0.62532115        nan        nan 0.62532115 0.62424282\n",
      "        nan 0.62532115 0.62532115 0.62575251        nan 0.6251054\n",
      "        nan 0.62402707 0.6251054         nan 0.62575251        nan\n",
      "        nan        nan 0.6251054  0.62532115 0.62575251 0.6251054\n",
      " 0.62532115 0.62510554 0.62726164 0.62510554 0.6251054         nan\n",
      " 0.62575251 0.62575251 0.62532115        nan 0.62014483        nan\n",
      " 0.6251054  0.62532115        nan        nan 0.62532115 0.61885089\n",
      " 0.6251054         nan 0.62532115 0.62532115 0.6190665         nan\n",
      " 0.6190665         nan 0.6251054  0.62510554 0.62510554        nan\n",
      " 0.6251054         nan        nan        nan 0.62532115 0.5718247\n",
      " 0.62575251        nan        nan 0.6251054  0.62510554 0.62532115\n",
      "        nan        nan        nan 0.62532115 0.62812436 0.62532115\n",
      "        nan        nan 0.6251054         nan 0.62575251 0.62532115\n",
      " 0.62575251        nan        nan 0.62532115 0.62532115        nan\n",
      "        nan 0.6251054  0.62424282 0.6251054  0.62575251 0.62036058\n",
      "        nan 0.62532115        nan 0.61885089        nan        nan\n",
      "        nan 0.62532115 0.62532115        nan 0.62532115        nan\n",
      "        nan 0.62532115 0.6251054  0.62057619        nan        nan\n",
      " 0.62532115 0.6251054  0.62726164        nan 0.6251054  0.61885089\n",
      "        nan 0.6251054  0.62812436 0.62532115 0.6251054  0.62812436\n",
      " 0.6251054         nan        nan 0.6251054         nan 0.62532115\n",
      " 0.62812436 0.6251054  0.62812436 0.6251054  0.62532115 0.62575251\n",
      " 0.6251054  0.61885089        nan        nan        nan 0.6251054\n",
      " 0.62532115        nan 0.62532115 0.6251054  0.62812436        nan\n",
      "        nan 0.62532115        nan        nan 0.61885089 0.6251054\n",
      " 0.6251054  0.62532115 0.6251054  0.62532115 0.62036044 0.62532115\n",
      " 0.62532115 0.62575251        nan 0.57160895        nan 0.62532115\n",
      " 0.62424282 0.62402707 0.62532115 0.62510554        nan 0.55694411\n",
      " 0.6251054  0.61885089        nan        nan 0.62532115 0.62510554\n",
      "        nan 0.61885089 0.62510554 0.61885089 0.62036044 0.6251054\n",
      "        nan        nan 0.62726164        nan        nan        nan\n",
      " 0.62402707        nan 0.62510554 0.62402707 0.62532115        nan\n",
      " 0.6251054  0.62532115 0.6251054  0.62036044        nan        nan\n",
      " 0.6251054         nan        nan        nan 0.62532115 0.62532115\n",
      " 0.6251054  0.62532115 0.5718247         nan 0.6251054         nan\n",
      " 0.62510554        nan 0.62036044        nan        nan 0.62402707\n",
      " 0.6251054         nan        nan        nan 0.62532115 0.6251054\n",
      " 0.62575251 0.62402707        nan        nan 0.62812436 0.62726164\n",
      "        nan 0.62014483        nan 0.55672836 0.62532115 0.62812436\n",
      " 0.62424282 0.62424282        nan        nan 0.62402707 0.6251054\n",
      "        nan 0.62510554 0.62812436        nan 0.62510554        nan\n",
      "        nan 0.6251054         nan 0.6251054  0.55672836 0.6251054\n",
      " 0.62532115 0.62510554 0.62036044        nan 0.6251054  0.62532115\n",
      " 0.57160895 0.62532115        nan        nan        nan 0.6251054\n",
      "        nan 0.62532115 0.6251054  0.62532115 0.6251054  0.62532115\n",
      " 0.62532115 0.6251054  0.62036044 0.6251054  0.62014483        nan\n",
      "        nan 0.6251054  0.6251054  0.62532115 0.6251054         nan\n",
      "        nan        nan 0.62575251 0.62575251        nan        nan\n",
      "        nan        nan 0.57160895 0.62532115 0.62532115 0.55694411\n",
      " 0.6251054         nan        nan        nan        nan 0.6251054\n",
      "        nan        nan        nan        nan 0.62575251        nan\n",
      "        nan        nan        nan        nan 0.6251054  0.62532115\n",
      " 0.62532115        nan        nan 0.61885089        nan        nan\n",
      " 0.62532115 0.6251054         nan 0.62532115 0.62532115        nan\n",
      " 0.57160895 0.6251054         nan 0.62532115        nan 0.57160895\n",
      "        nan 0.62532115 0.62402707 0.62402707 0.62402707        nan\n",
      "        nan        nan        nan        nan 0.62532115 0.6251054\n",
      " 0.6190665  0.62575251 0.62424282 0.62532115        nan 0.6251054\n",
      "        nan        nan        nan 0.62532115 0.62532115 0.62532115\n",
      " 0.62575251        nan 0.62532115 0.62036044        nan 0.62532115\n",
      " 0.6251054  0.62424282 0.6251054  0.6251054         nan 0.6251054\n",
      " 0.6251054  0.57204045 0.62532115        nan 0.62575251 0.6251054\n",
      "        nan        nan 0.62510554        nan 0.6251054         nan\n",
      " 0.62575251 0.6251054  0.6251054         nan 0.62402707 0.57160895\n",
      "        nan 0.62532115        nan        nan 0.6190665  0.6251054\n",
      "        nan        nan 0.6251054  0.62532115        nan 0.62510554\n",
      "        nan        nan 0.62532115        nan 0.62812436        nan\n",
      "        nan        nan        nan 0.6251054  0.62812436 0.6251054\n",
      "        nan 0.61885089 0.62575251 0.62575251 0.62036044 0.62532115\n",
      "        nan 0.6251054         nan 0.62726164 0.62510554 0.62532115\n",
      " 0.62402707        nan        nan 0.6251054  0.62532115 0.62532115\n",
      "        nan 0.62510554]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.628124358926052\n",
      "... with parameters: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 334, 'C': 0.01}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {'C':[0.01,0.1,1,2,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(250,500)\n",
    "                  \n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator =log_reg, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1  # n_jobs=-1 will utilize all available CPUs \n",
    "                                )\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "best_log_reg = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fad7ba9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 600 candidates, totalling 1800 fits\n",
      "The best recall score is 0.6283401086563648\n",
      "... with parameters: {'C': 0.01, 'max_iter': 34, 'penalty': 'l2', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 3\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength,min_regulization_strength+0.5), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-300,min_iter+300)\n",
    "}\n",
    "\n",
    "log_reg =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = log_reg, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,error_score='raise' # n_jobs=-1 will utilize all available CPUs \n",
    "                )\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_log_reg = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b304e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7023333 Precision=0.3916374 Recall=0.6135000 F1=0.4780830\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "Recall_logistic= {TP/(TP+FN)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ac0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"logistic using random & grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f9fd0",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ac146",
   "metadata": {},
   "source": [
    "### using RandomSearch and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {'C':np.arange(0.1,100,10),  #  regularization parameter.\n",
    "               'kernel':['linear', 'rbf','poly'],\n",
    "              'gamma':['scale','auto'],\n",
    "              'degree':np.arange(1,10), #degree is for the polynomial kernal\n",
    "              'coef0':np.arange(1,10) #coef0 is for the polynomial kernal\n",
    "                  \n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator =svc, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1  # n_jobs=-1 will utilize all available CPUs \n",
    "                                )\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "best_svc = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 3\n",
    "best_kernel = rand_search.best_params_['kernel']\n",
    "best_gamma = rand_search.best_params_['gamma']\n",
    "min_regulization=rand_search.best_params_['C']\n",
    "best_degree = rand_search.best_params_['degree']\n",
    "best_coef0=rand_search.best_params_['coef0']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization-3,min_regulization+3), \n",
    "               'kernel':[best_kernel],\n",
    "              'gamma':[best_gamma],\n",
    "              'degree': np.arange(best_degree-1,best_degree+1),\n",
    "            'coef0': np.arange(best_coef0-3,best_coef0+3)\n",
    "}\n",
    "\n",
    "svm_grid =  SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_grid, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1 # n_jobs=-1 will utilize all available CPUs \n",
    "                )\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_svm = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "Recall_SVM = {TP/(TP+FN)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0395038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"svm using Random & Grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb4009",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f2e73",
   "metadata": {},
   "source": [
    "We can observe that the recall score of decision tree and logistic regression using random search and grid search is 0.60 and 0.62 respectively which is almost equal. Comming to Support Vector Machines we are unable to get the result because of (incompatability of the system). So in this business problem we are mostly focussed on true positives(TP)(gives number of times the model correctly predicts a default payment) and False negatives(FN)(gives the number of times the model incorrectly predicts a non-default payment when the actual payment is a default).\n",
    "\n",
    "So when compared the best AI model developed to detect the default payments of both TP and FN is logisitic regression model.\n",
    "This model can detect the solution with less FN and more TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cda9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
